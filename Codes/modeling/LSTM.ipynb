{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0j3iaqPAIQ0T7rpliyzst"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KdpBz0vm6M8e"},"outputs":[],"source":["!pip3 install konlpy"]},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","tokenizer = Kkma()"],"metadata":{"id":"h4U07GCP-ANw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd \n","import numpy as np"],"metadata":{"id":"9JhfWGsl9_9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data = pd.read_csv(\"training.csv\")\n","training_data['X'] = training_data['X'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \",regex=True)\n","training_data['X'] = training_data['X'].str.replace('^ +', \"\",regex=True) # 공백은 empty 값으로 변경\n","training_data['X'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","training_data = training_data.dropna(how='any') # Null 값 제거\n","\n","validation_data = pd.read_csv(\"validation.csv\")\n","validation_data['X'] = validation_data['X'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \",regex=True)\n","validation_data['X'] = validation_data['X'].str.replace('^ +', \"\",regex=True) # 공백은 empty 값으로 변경\n","validation_data['X'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","validation_data = validation_data.dropna(how='any') # Null 값 제거\n","\n","test_data = pd.read_csv(\"test.csv\")\n","test_data['X'] = test_data['X'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \",regex=True)\n","test_data['X'] = test_data['X'].str.replace('^ +', \"\",regex=True) # 공백은 empty 값으로 변경\n","test_data['X'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","test_data = test_data.dropna(how='any') # Null 값 제거\n","\n","practice_data = pd.read_csv(\"practice.csv\")\n","practice_data['X'] = practice_data['X'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \",regex=True)\n","practice_data['X'] = practice_data['X'].str.replace('^ +', \"\",regex=True) # 공백은 empty 값으로 변경\n","practice_data['X'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","practice_data = practice_data.dropna(how='any') # Null 값 제거"],"metadata":{"id":"C1gRoTeb-Cpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Lyric_train = training_data['X'].values\n","Lyric_validation = validation_data['X'].values\n","Lyric_test = test_data['X'].values\n","Lyric_practice = practice_data['X'].values\n","y_train_temp = training_data['y'].values\n","y_validation_temp = validation_data['y'].values\n","y_test_temp = test_data['y'].values\n","y_practice_temp = practice_data['y'].values\n","\n","print(\"# split done\")"],"metadata":{"id":"OA9CTI2TWr2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_temp = []\n","X_validation_temp = []\n","X_test_temp = []\n","X_practice_temp = []\n","\n","for i in Lyric_train:\n","    token = tokenizer.morphs(i)\n","    X_train_temp.append(token)\n","for i in Lyric_validation:\n","    token = tokenizer.morphs(i)\n","    X_validation_temp.append(token)\n","for i in Lyric_test:\n","    token = tokenizer.morphs(i)\n","    X_test_temp.append(token)\n","for i in Lyric_practice:\n","    token = tokenizer.morphs(i)\n","    X_practice_temp.append(token)\n","\n","print(\"# tokenization done\")"],"metadata":{"id":"S5MgLTjRXF6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","tokenizer = Kkma()\n","X_practice_temp = []\n","Lyric_practice = practice_data['X'].values\n","for i in Lyric_practice:\n","    token = tokenizer.morphs(i)\n","    X_practice_temp.append(token)"],"metadata":{"id":"eBo0uCwJaEH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# One-Hot encoding 결과 구현\n","\n","y_train = pd.get_dummies(y_train_temp).values\n","y_validation = pd.get_dummies(y_validation_temp).values\n","y_test = pd.get_dummies(y_test_temp).values\n","y_practice = pd.get_dummies(y_practice_temp).values"],"metadata":{"id":"wIEEyHA9nMsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#New Learning\n","\n","X_train = X_train_temp\n","X_validation = X_validation_temp\n","X_test = X_test_temp\n","X_practice = X_practice_temp"],"metadata":{"id":"A7aiMy-Af1FI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","count = 0\n","for word, word_count in tokenizer.word_counts.items():\n","    if word_count > 1:\n","        count += 1\n","print(count)"],"metadata":{"id":"lhPGkpc2GEmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = 11335\n","tokenizer = Tokenizer(vocab_size)\n","tokenizer.fit_on_texts(X_train)"],"metadata":{"id":"Hyf6TkDYGEf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_validation = tokenizer.texts_to_sequences(X_validation)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","X_practice = tokenizer.texts_to_sequences(X_practice)\n","\n","print(\"# int_encoding done\")\n","max_len = 100\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_validation = pad_sequences(X_validation, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","X_practice = pad_sequences(X_practice, maxlen=max_len)"],"metadata":{"id":"yxJJTQw1GENO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Embedding, Dropout\n","\n","embedding_dim = 100\n","hidden_units = 128\n","num_classes = 4\n","\n","# 모델 구축\n","# 레이어들을 쌓을 모델을 생성\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(LSTM(hidden_units))\n","model.add(Dropout(0.3))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# 테스트 데이터 손실함수값(val_acc)이 patience회 이상 연속 증가하면 학습을 조기 종료하는 콜백\n","early_stop = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=10)\n","# 훈련 도중 테스트 데이터 정확도(val_acc)가 높았던 순간을 체크포인트로 저장해 활용하는 콜백\n","model_check = ModelCheckpoint('the_best.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"],"metadata":{"id":"jVWDfSzcOV8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hist = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=50, batch_size=16, callbacks=[early_stop, model_check])"],"metadata":{"id":"yE9XYKXWQ4pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"UNvHqeGWQnDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, len(hist.history['acc']) + 1)\n","plt.plot(epochs, hist.history['loss'])\n","plt.plot(epochs, hist.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","plt.plot(epochs, hist.history['acc'])\n","plt.plot(epochs, hist.history['val_acc'])\n","plt.title('Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"metadata":{"id":"zEADYorZIxce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정확도 측정\n","# 출력하면 [loss, acc]\n","from tensorflow.keras.models import load_model\n","\n","loaded_model = load_model('the_best.h5')\n","print(\"테스트 정확도 : %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"metadata":{"id":"LzIKIOXdOgvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confusion matrix 사용을 위한 라이브러리\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n","import itertools\n","\n","# confusion matrix 그리는 함수 \n","def plot_confusion_matrix(con_mat, labels, title='Confusion Matrix', cmap=plt.cm.get_cmap('Blues'), normalize=False):\n","    plt.imshow(con_mat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    marks = np.arange(len(labels))\n","    nlabels = []\n","    for k in range(len(con_mat)):\n","        n = sum(con_mat[k])\n","        nlabel = '{0}(n={1})'.format(labels[k],n)\n","        nlabels.append(nlabel)\n","    plt.xticks(marks, labels)\n","    plt.yticks(marks, nlabels)\n","\n","    thresh = con_mat.max() / 2.\n","    if normalize:\n","        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n","            plt.text(j, i, '{0}'.format(con_mat[i, j]), horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n","    else:\n","        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n","            plt.text(j, i, con_mat[i, j], horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","\n","predictions = loaded_model.predict(X_test)\n","labels = ['sad', 'fear', 'happiness', 'anger']\n","# 예측값과 참값 \n","pred_labels = np.argmax(predictions, axis=1)\n","y_ = []\n","for i in y_test:\n","    if i[0] == 1:\n","        y_.append(0)\n","    elif i[1] == 1:\n","        y_.append(1)\n","    elif i[2] == 1:\n","        y_.append(2)\n","    else:\n","        y_.append(3)\n","true_labels = y_\n","#메인 실행 \n","confusion_matrix = confusion_matrix(true_labels, pred_labels)\n","plot_confusion_matrix(confusion_matrix, labels=labels, normalize=True)"],"metadata":{"id":"VNwIzTnwHHYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(confusion_matrix)"],"metadata":{"id":"JJGa142EHHRo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#precision\n","\n","print(\"precision_score : \", precision_score(true_labels, pred_labels, average=\"weighted\"))\n","\n","#recall\n","\n","print(\"recall_score : \", recall_score(true_labels, pred_labels, average=\"weighted\"))\n","\n","#f1score\n","\n","print(\"f1_score : \", f1_score(true_labels, pred_labels, average=\"weighted\"))"],"metadata":{"id":"Cn8fNgVnvXid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Multi Demo"],"metadata":{"id":"24FSANWRC6Yq"}},{"cell_type":"code","source":["y_pred = model.predict(X_practice)\n","result = []\n","for i in range(len(y_pred)):\n","    if np.argmax(y_pred[i]) == 0:\n","        result.append(\"슬픔\")\n","    elif np.argmax(y_pred[i]) == 1:\n","        result.append(\"두렵\")\n","    elif np.argmax(y_pred[i]) == 2:\n","        result.append(\"행복\")\n","    else:\n","        result.append(\"분노\")\n","\n","for i in range(len(practice_data)):\n","    print(\"제목 : \", practice_data['제목'][i], \", 기존 감정 : \", practice_data['y'][i], \", 예측 감정 : \", result[i])"],"metadata":{"id":"613Ogn22C5oW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Binary"],"metadata":{"id":"2c23qqw_X8nB"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Embedding, Dropout\n","\n","embedding_dim = 100\n","hidden_units = 128\n","num_classes = 2\n","\n","# 모델 구축\n","# 레이어들을 쌓을 모델을 생성\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(LSTM(hidden_units))\n","model.add(Dropout(0.3))\n","model.add(Dense(num_classes, activation='sigmoid'))\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# 테스트 데이터 손실함수값(val_acc)이 patience회 이상 연속 증가하면 학습을 조기 종료하는 콜백\n","early_stop = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=15)\n","# 훈련 도중 테스트 데이터 정확도(val_acc)가 높았던 순간을 체크포인트로 저장해 활용하는 콜백\n","model_check = ModelCheckpoint('the_best2.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"],"metadata":{"id":"hTe1lJRzX77R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hist = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=50, batch_size=16, callbacks=[early_stop, model_check])"],"metadata":{"id":"oAA9o1KCYCGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정확도 측정\n","# 출력하면 [loss, acc]\n","from tensorflow.keras.models import load_model\n","\n","loaded_model = load_model('the_best2.h5')\n","print(\"테스트 정확도 : %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"metadata":{"id":"fFgYkfH7h041"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, len(hist.history['acc']) + 1)\n","plt.plot(epochs, hist.history['loss'])\n","plt.plot(epochs, hist.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","plt.plot(epochs, hist.history['acc'])\n","plt.plot(epochs, hist.history['val_acc'])\n","plt.title('Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"metadata":{"id":"6-M-L2eeYF-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confusion matrix 사용을 위한 라이브러리\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n","import itertools\n","\n","# confusion matrix 그리는 함수 \n","def plot_confusion_matrix(con_mat, labels, title='Confusion Matrix', cmap=plt.cm.get_cmap('Blues'), normalize=False):\n","    plt.imshow(con_mat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    marks = np.arange(len(labels))\n","    nlabels = []\n","    for k in range(len(con_mat)):\n","        n = sum(con_mat[k])\n","        nlabel = '{0}(n={1})'.format(labels[k],n)\n","        nlabels.append(nlabel)\n","    plt.xticks(marks, labels)\n","    plt.yticks(marks, nlabels)\n","\n","    thresh = con_mat.max() / 2.\n","    if normalize:\n","        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n","            plt.text(j, i, '{0}'.format(con_mat[i, j]), horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n","    else:\n","        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n","            plt.text(j, i, con_mat[i, j], horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","\n","predictions = loaded_model.predict(X_test)\n","labels = ['minus', 'plus']\n","# 예측값과 참값 \n","pred_labels = np.argmax(predictions, axis=1)\n","y_ = []\n","for i in y_test:\n","    if i[0] == 1:\n","        y_.append(0)\n","    else:\n","        y_.append(1)\n","true_labels = y_\n","#메인 실행 \n","confusion_matrix = confusion_matrix(true_labels, pred_labels)\n","plot_confusion_matrix(confusion_matrix, labels=labels, normalize=True)"],"metadata":{"id":"jkb0Aa8yYKSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#precision\n","\n","print(\"precision_score : \", precision_score(true_labels, pred_labels, average=\"weighted\"))\n","\n","#recall\n","\n","print(\"recall_score : \", recall_score(true_labels, pred_labels, average=\"weighted\"))\n","\n","#f1scord\n","\n","print(\"f1_score : \", f1_score(true_labels, pred_labels, average=\"weighted\"))"],"metadata":{"id":"rpQpnkeDYM6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Binary Demo"],"metadata":{"id":"IWkEwVP0C-wn"}},{"cell_type":"code","source":["y_pred = model.predict(X_practice)\n","result = []\n","for i in range(len(y_pred)):\n","    if np.argmax(y_pred[i]) == 0:\n","        result.append(\"부정\")\n","    else:\n","        result.append(\"긍정\")\n","\n","for i in range(len(practice_data)):\n","    print(\"제목 : \", practice_data['제목'][i], \", 기존 감정 : \", practice_data['y'][i], \", 예측 감정 : \", result[i])"],"metadata":{"id":"mrMhrTbUkJDO"},"execution_count":null,"outputs":[]}]}